{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtbe Title and Date Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Packes import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#2. Youtube title & date crawler\n",
    "def crawling(url):\n",
    "    print(\"Let's crawling!\")\n",
    "    \n",
    "    #크롬 드라이버 설정\n",
    "    driver = webdriver.Chrome('C:/selenium/chromedriver.exe')\n",
    "    driver.get(url)\n",
    "\n",
    "    #페이지 넘김 횟수 지정\n",
    "    time.sleep(5)\n",
    "    endk = 100\n",
    "    while endk:\n",
    "        driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "        time.sleep(0.3)\n",
    "        endk -= 1\n",
    "    \n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "    \n",
    "    #타이틀 추출\n",
    "    all_title = soup.find_all('a','yt-simple-endpoint style-scope ytd-grid-video-renderer')\n",
    "    title = [soup.find_all('a','yt-simple-endpoint style-scope ytd-grid-video-renderer')[n].string for n in range(0,len(all_title))]\n",
    "    print(\"title example \\n\", title[:10])\n",
    "    \n",
    "    #조회수, 게시일자 추출\n",
    "    c = soup.find_all('span','style-scope ytd-grid-video-renderer')\n",
    "    view_num = [soup.find_all('span','style-scope ytd-grid-video-renderer')[n].string for n in range(0,len(c))]\n",
    "    \n",
    "    #게시일자만 분리\n",
    "    date = []\n",
    "    for i in range(len(view_num)):\n",
    "        if i % 2 == 1:\n",
    "            date.append(view_num[i])\n",
    "    print(\"date example \\n\", date[:10])\n",
    "                   \n",
    "    #개수 맞는지 확인하기\n",
    "    print(\"number of title : \", len(title))\n",
    "    print(\"number of date : \", len(date))\n",
    "    \n",
    "    return date, title\n",
    "\n",
    "\n",
    "def data_save(date, title, filename):\n",
    "    #하나로 합치기\n",
    "    data = []\n",
    "    for i in range(0, len(title)):\n",
    "        temp = []\n",
    "        temp.append(title[i])\n",
    "        temp.append(date[i])\n",
    "        data.append(temp)\n",
    "\n",
    "    print(\"end.\")\n",
    "    print(filename+\" data rows : \", len(data))\n",
    "    #데이터 프레임 형태로 변환 후 저장\n",
    "    data_df = pd.DataFrame(data, columns=['title', 'date'])\n",
    "    data_df.to_csv(\"C:/Users/강미경/Desktop/령아 ,,,,!/\"+filename+\".csv\", encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "def Youtube_crawling(url, filename):\n",
    "    date, title = crawling(url)\n",
    "    data_save(date, title, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Youtube List title crawler\n",
    "def list_crawling(url):\n",
    "    print(\"\\n\\nLet's crawling!\")\n",
    "    \n",
    "    #크롬 드라이버 설정\n",
    "    driver = webdriver.Chrome('C:/selenium/chromedriver.exe')\n",
    "    driver.get(url)\n",
    "\n",
    "    #페이지 넘김 횟수 지정\n",
    "    time.sleep(5)\n",
    "    endk = 35\n",
    "    while endk:\n",
    "        driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "        time.sleep(3)\n",
    "        print(\"scroll : \", endk)\n",
    "        endk -= 1\n",
    "    \n",
    "    print(\"end crawling!\")\n",
    "\n",
    "    print(\"Page loading...\")\n",
    "    page = driver.page_source\n",
    "    print(\"page size : \", len(page))\n",
    "    \n",
    "    print(\"\\nParsing...\")\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "\n",
    "    #타이틀 추출\n",
    "    all_title = soup.find_all('span','style-scope ytd-playlist-video-renderer')\n",
    "    title = [soup.find_all('span','style-scope ytd-playlist-video-renderer')[n].string.strip().replace(\" / JTBC 뉴스룸\", \"\", 1) for n in range(0, len(all_title))]\n",
    "    print(\"title example \\n\", title[:10])\n",
    "    \n",
    "    #개수 맞는지 확인하기\n",
    "    print(\"number of title : \", len(title))\n",
    "    return title\n",
    "\n",
    "\n",
    "def data_list_save(title, filename):\n",
    "    print(filename+\" data rows : \", len(title))\n",
    "    #데이터 프레임 형태로 변환 후 저장\n",
    "    data_df = pd.DataFrame(title, columns=['title'])\n",
    "    data_df.to_csv(\"C:/Users/강미경/Desktop/령아 ,,,,!/\"+filename+\".csv\", encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "def Youtube_list_crawling(url, filename):\n",
    "    title = list_crawling(url)\n",
    "    data_list_save(title, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://www.youtube.com/playlist?list=PLUHG6IBxDr3h61KfTrExWlEdIp6uC-4O0',\n",
       "  'sbs']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_list = [\n",
    "            [\"https://www.youtube.com/playlist?list=PLUHG6IBxDr3h61KfTrExWlEdIp6uC-4O0\", \"sbs\"]]\n",
    "urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/playlist?list=PLUHG6IBxDr3h61KfTrExWlEdIp6uC-4O0 sbs\n",
      "\n",
      "\n",
      "Let's crawling!\n",
      "scroll :  35\n",
      "scroll :  34\n",
      "scroll :  33\n",
      "scroll :  32\n",
      "scroll :  31\n",
      "scroll :  30\n",
      "scroll :  29\n",
      "scroll :  28\n",
      "scroll :  27\n",
      "scroll :  26\n",
      "scroll :  25\n",
      "scroll :  24\n",
      "scroll :  23\n",
      "scroll :  22\n",
      "scroll :  21\n",
      "scroll :  20\n",
      "scroll :  19\n",
      "scroll :  18\n",
      "scroll :  17\n",
      "scroll :  16\n",
      "scroll :  15\n",
      "scroll :  14\n",
      "scroll :  13\n",
      "scroll :  12\n",
      "scroll :  11\n",
      "scroll :  10\n",
      "scroll :  9\n",
      "scroll :  8\n",
      "scroll :  7\n",
      "scroll :  6\n",
      "scroll :  5\n",
      "scroll :  4\n",
      "scroll :  3\n",
      "scroll :  2\n",
      "scroll :  1\n",
      "end crawling!\n",
      "Page loading...\n",
      "page size :  24570323\n",
      "\n",
      "Parsing...\n",
      "title example \n",
      " ['제주도, 해열제 먹으며 여행한 코로나19 확진 관광객에 손배소송 / SBS', '베이징 집단감염 지속에 중위험 지역 4곳 고위험으로 상향 / SBS', \"6/22(월) '코로나19' 중앙방역대책본부 브리핑 / SBS\", '방판업체발 집단감염 전국 확산…대전시 \"무료 진단검사\" / SBS', \"6/22(월) '코로나19 대유행 대비 방안' 관련 서울시 브리핑 / SBS\", '코로나19 신규 확진 17명… 누적 12,438명 / SBS', \"6/22(월) '코로나19' 중앙재난안전대책본부 브리핑 / SBS\", \"대전 방판업체발 감염, 수도권 · 익산 · 광주 '확산일로' / SBS\", '농번기 외국인 입국↑…비자 발급 · 항공편 운항 제한 / SBS', '\"최악 상황 대비하라\"…코로나19 퇴원 기준 완화 권고 / SBS']\n",
      "number of title :  3286\n",
      "sbs data rows :  3286\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(urls_list)):\n",
    "    url = urls_list[i][0]\n",
    "    filename = urls_list[i][1]\n",
    "    print(url, filename)\n",
    "    Youtube_list_crawling(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://www.youtube.com/user/JTBC10news/videos', 'jtbc'],\n",
       " ['https://www.youtube.com/user/sbsnews8/videos', 'sbs'],\n",
       " ['https://www.youtube.com/user/tvchanews/videos', '채널a']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [[\"https://www.youtube.com/user/JTBC10news/videos\", \"jtbc\"],\n",
    "        [\"https://www.youtube.com/user/sbsnews8/videos\", \"sbs\"],\n",
    "        [\"https://www.youtube.com/user/tvchanews/videos\", \"채널a\"]]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/user/JTBC10news/videos jtbc\n",
      "Let's crawling!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(urls)):\n",
    "    url = urls[i][0]\n",
    "    filename = urls[i][1]\n",
    "    print(url, filename)\n",
    "    Youtube_crawling(url, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
